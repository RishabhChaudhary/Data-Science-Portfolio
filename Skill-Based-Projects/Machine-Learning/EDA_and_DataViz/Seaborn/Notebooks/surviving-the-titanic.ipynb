{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<html>\n    <body>\n        <h1 class=\"alert alert-info\" style=\"text-align: center;\">Surviving the Titanic: A Machine Learning Approach to Predicting Passenger Survival</h1>\n        <h2 id=\"contents\">Table of Contents</h2>\n        <ol>\n            <a href=\"#section1\"><li>Importing libraries and loading the dataset</li></a>\n            <a href=\"#section2\"><li>Exploring the dataset</li></a>\n            <a href=\"#section3\"><li>Data cleaning</li></a>\n            <a href=\"#section4\"><li>Exploratory data analysis</li></a>\n            <ol>\n                <a href=\"#sub_section1_1\"><li type=\"i\">Univariate analysis</li></a>\n                <a href=\"#sub_section1_2\"><li type=\"i\">Bivariate analysis</li></a>\n            </ol>        \n            <a href=\"#section5\"><li>Data Prepocessing</li></a>\n            <a href=\"#section6\"><li>Model Building and Evaluation</li></a>\n            <ol>\n                <a href=\"#sub_section2_1\"><li type=\"i\">KNN Classifier</li></a>\n                <a href=\"#sub_section2_2\"><li type=\"i\">Logistic Regreassion</li></a>\n                <a href=\"#sub_section2_3\"><li type=\"i\">Decision Tree Classifier</li></a>\n                <a href=\"#sub_section2_4\"><li type=\"i\">MPLC Classifier</li></a>\n                <a href=\"#sub_section2_5\"><li type=\"i\">Support Vector Machine</li></a>\n            </ol> \n        </ol>\n    </body>\n</html>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h2 id=\"section1\">1. Importing libraries and loading the dataset</h2>\n    <p>Let's start by importing the necessary libraries and loading the dataset.</p>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\nimport warnings\nimport re\n\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nwarnings.filterwarnings(action = 'ignore')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:32.736619Z","iopub.execute_input":"2023-05-21T21:12:32.737118Z","iopub.status.idle":"2023-05-21T21:12:34.248127Z","shell.execute_reply.started":"2023-05-21T21:12:32.737070Z","shell.execute_reply":"2023-05-21T21:12:34.246377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the data\ndf_train = pd.read_csv('../input/titanic/train.csv')\ndf_test = pd.read_csv('../input/titanic/test.csv')\ndf_submission  = pd.read_csv('../input/titanic/gender_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.250284Z","iopub.execute_input":"2023-05-21T21:12:34.251067Z","iopub.status.idle":"2023-05-21T21:12:34.295020Z","shell.execute_reply.started":"2023-05-21T21:12:34.250987Z","shell.execute_reply":"2023-05-21T21:12:34.293262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h2 id=\"section2\">2. Exploring the dataset</h2>\n    <p>Let's explore the datasets:</p>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contens\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Shape of the data\nprint('Shape of the train data: %s', df_train.shape)\nprint('Shape of the test data: %s', df_test.shape)\nprint('Shape of the submission data: %s', df_submission.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.297488Z","iopub.execute_input":"2023-05-21T21:12:34.298015Z","iopub.status.idle":"2023-05-21T21:12:34.308730Z","shell.execute_reply.started":"2023-05-21T21:12:34.297966Z","shell.execute_reply":"2023-05-21T21:12:34.306754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample train data\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.313173Z","iopub.execute_input":"2023-05-21T21:12:34.313617Z","iopub.status.idle":"2023-05-21T21:12:34.356247Z","shell.execute_reply.started":"2023-05-21T21:12:34.313572Z","shell.execute_reply":"2023-05-21T21:12:34.353241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Smaple of test data\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.358606Z","iopub.execute_input":"2023-05-21T21:12:34.360258Z","iopub.status.idle":"2023-05-21T21:12:34.377868Z","shell.execute_reply.started":"2023-05-21T21:12:34.360210Z","shell.execute_reply":"2023-05-21T21:12:34.376488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample submission data\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.379594Z","iopub.execute_input":"2023-05-21T21:12:34.380438Z","iopub.status.idle":"2023-05-21T21:12:34.398045Z","shell.execute_reply.started":"2023-05-21T21:12:34.380389Z","shell.execute_reply":"2023-05-21T21:12:34.396211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Let's explore the train dataset to get a better understanding of its structure and content:</p>","metadata":{}},{"cell_type":"code","source":"# Data types\ndf_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.400211Z","iopub.execute_input":"2023-05-21T21:12:34.401080Z","iopub.status.idle":"2023-05-21T21:12:34.418855Z","shell.execute_reply.started":"2023-05-21T21:12:34.401017Z","shell.execute_reply":"2023-05-21T21:12:34.415889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary statistics\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.420459Z","iopub.execute_input":"2023-05-21T21:12:34.420862Z","iopub.status.idle":"2023-05-21T21:12:34.468134Z","shell.execute_reply.started":"2023-05-21T21:12:34.420823Z","shell.execute_reply":"2023-05-21T21:12:34.466540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unique values in each column\ndf_train.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.469250Z","iopub.execute_input":"2023-05-21T21:12:34.469541Z","iopub.status.idle":"2023-05-21T21:12:34.491304Z","shell.execute_reply.started":"2023-05-21T21:12:34.469511Z","shell.execute_reply":"2023-05-21T21:12:34.489285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h3 id=\"section3\">3. Data cleaning</h3>\n    <p>Nice! We have a dataset with <b>891</b> rows and <b>12</b> columns. Let's clean the dataset by handling missing values, duplicates, irrelevant columns, and converting data types.</p>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Let's create a copy of the train and test data to perform data cleaning\ndf_train_copy = df_train.copy()\ndf_test_copy = df_test.copy()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.497795Z","iopub.execute_input":"2023-05-21T21:12:34.498233Z","iopub.status.idle":"2023-05-21T21:12:34.506890Z","shell.execute_reply.started":"2023-05-21T21:12:34.498190Z","shell.execute_reply":"2023-05-21T21:12:34.504496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in train data\ndf_train_copy.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.509523Z","iopub.execute_input":"2023-05-21T21:12:34.510507Z","iopub.status.idle":"2023-05-21T21:12:34.526420Z","shell.execute_reply.started":"2023-05-21T21:12:34.510447Z","shell.execute_reply":"2023-05-21T21:12:34.525256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in test data\ndf_test_copy.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.528153Z","iopub.execute_input":"2023-05-21T21:12:34.528566Z","iopub.status.idle":"2023-05-21T21:12:34.543790Z","shell.execute_reply.started":"2023-05-21T21:12:34.528528Z","shell.execute_reply":"2023-05-21T21:12:34.542125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>We have ver high number of missing values in <b>Cabin</b> followed by <b>Age</b> and just 2 in <b>Embarked</b> column.</p>\n<p>Lets impute missing values in <b>Embarked</b> column by mode. and check the other columns if they have any patterns</p>","metadata":{}},{"cell_type":"code","source":"# Impute missing values in Age column with median\n# df_train_copy['Age'] = df_train_copy['Age'].fillna(df_train_copy['Age'].median())\n# df_test_copy['Age'] = df_test_copy['Age'].fillna(df_test_copy['Age'].median())\n\n# Impute missing values in Embarked column with mode\ndf_train_copy['Embarked'] = df_train_copy['Embarked'].fillna(df_train_copy['Embarked'].mode()[0])\ndf_test_copy['Embarked'] = df_test_copy['Embarked'].fillna(df_test_copy['Embarked'].mode()[0])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.546382Z","iopub.execute_input":"2023-05-21T21:12:34.547067Z","iopub.status.idle":"2023-05-21T21:12:34.556146Z","shell.execute_reply.started":"2023-05-21T21:12:34.547036Z","shell.execute_reply":"2023-05-21T21:12:34.554968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Here empty values in <b>Cabin</b> column may indicate that passenger didn't have a cabin. So we can explore further if emplty values in cabin has relation with Survived</p>","metadata":{}},{"cell_type":"code","source":"# Check if there is any relations between the missing values in Cabin column and Survived column\ndf_train_copy[df_train_copy['Cabin'].isna()]['Survived'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.557987Z","iopub.execute_input":"2023-05-21T21:12:34.558525Z","iopub.status.idle":"2023-05-21T21:12:34.574211Z","shell.execute_reply.started":"2023-05-21T21:12:34.558470Z","shell.execute_reply":"2023-05-21T21:12:34.573036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>We can clearly see that passengers with empty values in <b>Cabin</b> column have less chance of survival.</p>\n<p>Cabin has signficant effect on Survival soo let's create a new category for the missing Cabins called \"Missing\"</p>","metadata":{}},{"cell_type":"code","source":"# Impute missing values in Cabin column with 'Missing'\ndf_train_copy['Cabin'] = df_train_copy['Cabin'].fillna('Missing')\ndf_test_copy['Cabin'] = df_test_copy['Cabin'].fillna('Missing')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.576912Z","iopub.execute_input":"2023-05-21T21:12:34.577336Z","iopub.status.idle":"2023-05-21T21:12:34.588411Z","shell.execute_reply.started":"2023-05-21T21:12:34.577305Z","shell.execute_reply":"2023-05-21T21:12:34.587443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Now let's check what variable can impact Age. In the Name column we can see the Initial which can give some information about the Age, let's extract that.</p>","metadata":{}},{"cell_type":"code","source":"# Let's extract the name title from the Name column\ndf_train_copy['Name_Title'] = df_train_copy['Name'].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\ndf_test_copy['Name_Title'] = df_test_copy['Name'].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\ndf_train_copy['Name_Title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.590004Z","iopub.execute_input":"2023-05-21T21:12:34.591875Z","iopub.status.idle":"2023-05-21T21:12:34.618133Z","shell.execute_reply.started":"2023-05-21T21:12:34.591816Z","shell.execute_reply":"2023-05-21T21:12:34.616051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's combine the similar titles, we will combine Mlle, Jonkheer and Ms with Miss, Mme, Countess and Lady with Mrs and Rev, Dr, Col, Major, Don, Sir, Capt with Rare\ndf_train_copy['Name_Title'] = df_train_copy['Name_Title'].replace(['Mlle', 'Ms', 'Jonkheer'], 'Miss')\ndf_train_copy['Name_Title'] = df_train_copy['Name_Title'].replace(['Mme', 'Countess', 'Lady', 'Dona'], 'Mrs')\ndf_train_copy['Name_Title'] = df_train_copy['Name_Title'].replace(['Rev', 'Dr', 'Col', 'Major', 'Don', 'Sir', 'Capt'], 'Rare')\n\ndf_test_copy['Name_Title'] = df_test_copy['Name_Title'].replace(['Mlle', 'Ms', 'Jonkheer'], 'Miss')\ndf_test_copy['Name_Title'] = df_test_copy['Name_Title'].replace(['Mme', 'Countess', 'Lady', 'Dona'], 'Mrs')\ndf_test_copy['Name_Title'] = df_test_copy['Name_Title'].replace(['Rev', 'Dr', 'Col', 'Major', 'Don', 'Sir', 'Capt'], 'Rare')\n\ndf_train_copy['Name_Title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.622185Z","iopub.execute_input":"2023-05-21T21:12:34.623232Z","iopub.status.idle":"2023-05-21T21:12:34.650189Z","shell.execute_reply.started":"2023-05-21T21:12:34.623080Z","shell.execute_reply":"2023-05-21T21:12:34.648957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's see the Age of the passengers with respect to their Name_Title\ndf_train_copy.groupby('Name_Title')['Age'].median()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.652076Z","iopub.execute_input":"2023-05-21T21:12:34.653818Z","iopub.status.idle":"2023-05-21T21:12:34.667940Z","shell.execute_reply.started":"2023-05-21T21:12:34.653740Z","shell.execute_reply":"2023-05-21T21:12:34.666849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Great, now let's impute the missing values in Age column with the median of the respective Name_Title\ndf_train_copy.loc[(df_train_copy['Age'].isna()) & (df_train_copy['Name_Title'] == 'Master'), 'Age'] = df_train_copy[df_train_copy['Name_Title'] == 'Master']['Age'].median()\ndf_train_copy.loc[(df_train_copy['Age'].isna()) & (df_train_copy['Name_Title'] == 'Mr'), 'Age'] = df_train_copy[df_train_copy['Name_Title'] == 'Mr']['Age'].median()\ndf_train_copy.loc[(df_train_copy['Age'].isna()) & (df_train_copy['Name_Title'] == 'Mrs'), 'Age'] = df_train_copy[df_train_copy['Name_Title'] == 'Mrs']['Age'].median()\ndf_train_copy.loc[(df_train_copy['Age'].isna()) & (df_train_copy['Name_Title'] == 'Miss'), 'Age'] = df_train_copy[df_train_copy['Name_Title'] == 'Miss']['Age'].median()\ndf_train_copy.loc[(df_train_copy['Age'].isna()) & (df_train_copy['Name_Title'] == 'Rare'), 'Age'] = df_train_copy[df_train_copy['Name_Title'] == 'Rare']['Age'].median()\n\ndf_test_copy.loc[(df_test_copy['Age'].isna()) & (df_test_copy['Name_Title'] == 'Miss'), 'Age'] = df_test_copy[df_test_copy['Name_Title'] == 'Miss']['Age'].median()\ndf_test_copy.loc[(df_test_copy['Age'].isna()) & (df_test_copy['Name_Title'] == 'Mr'), 'Age'] = df_test_copy[df_test_copy['Name_Title'] == 'Mr']['Age'].median()\ndf_test_copy.loc[(df_test_copy['Age'].isna()) & (df_test_copy['Name_Title'] == 'Mrs'), 'Age'] = df_test_copy[df_test_copy['Name_Title'] == 'Mrs']['Age'].median()\ndf_test_copy.loc[(df_test_copy['Age'].isna()) & (df_test_copy['Name_Title'] == 'Master'), 'Age'] = df_test_copy[df_test_copy['Name_Title'] == 'Master']['Age'].median()\ndf_test_copy.loc[(df_test_copy['Age'].isna()) & (df_test_copy['Name_Title'] == 'Rare'), 'Age'] = df_test_copy[df_test_copy['Name_Title'] == 'Rare']['Age'].median()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.670374Z","iopub.execute_input":"2023-05-21T21:12:34.671293Z","iopub.status.idle":"2023-05-21T21:12:34.732281Z","shell.execute_reply.started":"2023-05-21T21:12:34.671242Z","shell.execute_reply":"2023-05-21T21:12:34.729241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let see if we still have any missing values in the train data\ndf_train_copy.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.734164Z","iopub.execute_input":"2023-05-21T21:12:34.734613Z","iopub.status.idle":"2023-05-21T21:12:34.748170Z","shell.execute_reply.started":"2023-05-21T21:12:34.734573Z","shell.execute_reply":"2023-05-21T21:12:34.746807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing missing values in Fare column with median in test data\ndf_test_copy['Fare'] = df_test_copy['Fare'].fillna(df_test_copy['Fare'].median())\n\n# Let see if we still have any missing values in test data\ndf_test_copy.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.749406Z","iopub.execute_input":"2023-05-21T21:12:34.750046Z","iopub.status.idle":"2023-05-21T21:12:34.764821Z","shell.execute_reply.started":"2023-05-21T21:12:34.750010Z","shell.execute_reply":"2023-05-21T21:12:34.763494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicates in train data\ndf_train_copy.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.766397Z","iopub.execute_input":"2023-05-21T21:12:34.767019Z","iopub.status.idle":"2023-05-21T21:12:34.790936Z","shell.execute_reply.started":"2023-05-21T21:12:34.766925Z","shell.execute_reply":"2023-05-21T21:12:34.788782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicates in test data\ndf_test_copy.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.792861Z","iopub.execute_input":"2023-05-21T21:12:34.793495Z","iopub.status.idle":"2023-05-21T21:12:34.806527Z","shell.execute_reply.started":"2023-05-21T21:12:34.793438Z","shell.execute_reply":"2023-05-21T21:12:34.804333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>No duplicates in the dataset! Let's move on to the next step.</p>","metadata":{}},{"cell_type":"markdown","source":"<p>Let's convert Survived, Pclass, Sex, SibSp, Parch, and Embarked to categorical variables.</p>","metadata":{}},{"cell_type":"code","source":"# Convert Name, Survived, Pclass, Sex, SibSp, Parch, Embarked, Ticket, Cabin to categorical variables in train data\ndf_train_copy['Survived'] = df_train_copy['Survived'].astype('category')\ndf_train_copy['Pclass'] = df_train_copy['Pclass'].astype('category')\ndf_train_copy['Sex'] = df_train_copy['Sex'].astype('category')\ndf_train_copy['SibSp'] = df_train_copy['SibSp'].astype('category')\ndf_train_copy['Parch'] = df_train_copy['Parch'].astype('category')\ndf_train_copy['Embarked'] = df_train_copy['Embarked'].astype('category')\ndf_train_copy['Ticket'] = df_train_copy['Ticket'].astype('category')\ndf_train_copy['Cabin'] = df_train_copy['Cabin'].astype('category')\ndf_train_copy['Name'] = df_train_copy['Name'].astype('category')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.808155Z","iopub.execute_input":"2023-05-21T21:12:34.808913Z","iopub.status.idle":"2023-05-21T21:12:34.853717Z","shell.execute_reply.started":"2023-05-21T21:12:34.808857Z","shell.execute_reply":"2023-05-21T21:12:34.850880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Name, Pclass, Sex, SibSp, Parch, Embarked, Ticket, Cabin to categorical variables in test data\ndf_test_copy['Pclass'] = df_test_copy['Pclass'].astype('category')\ndf_test_copy['Sex'] = df_test_copy['Sex'].astype('category')\ndf_test_copy['SibSp'] = df_test_copy['SibSp'].astype('category')\ndf_test_copy['Parch'] = df_test_copy['Parch'].astype('category')\ndf_test_copy['Embarked'] = df_test_copy['Embarked'].astype('category')\ndf_test_copy['Ticket'] = df_test_copy['Ticket'].astype('category')\ndf_test_copy['Cabin'] = df_test_copy['Cabin'].astype('category')\ndf_test_copy['Name'] = df_test_copy['Name'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.856570Z","iopub.execute_input":"2023-05-21T21:12:34.857301Z","iopub.status.idle":"2023-05-21T21:12:34.878580Z","shell.execute_reply.started":"2023-05-21T21:12:34.857221Z","shell.execute_reply":"2023-05-21T21:12:34.877355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h3 id=\"section4\">4. Exploratory data analysis</h3>\n    <p>Let's perform exploratory data analysis to extract insights from the cab trips dataset:</p>\n    <h4 id=\"sub_section1_1\" >i. Univariate analysis</h4>\n    <p>We will start by exploring the distribution of the numerical and categorical variables in the dataset:</p>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Function for calculating descriptives of numeric variable and plotting the distribution\ndef plot_dist(df, col, x_label, y_label, plot_title):\n    _min = df[col].min()\n    _max = df[col].max()\n    ran = df[col].max()-df[col].min()\n    mean = df[col].mean()\n    median = df[col].median()\n    st_dev = df[col].std()\n    skew = df[col].skew()\n    kurt = df[col].kurtosis()\n\n    # calculating points of standard deviation\n    points = mean-st_dev, mean+st_dev\n    sns.set_style('darkgrid')\n    plt.figure(figsize=(12,8))\n    sns.histplot(data=df, x=col, bins=30, kde=True, color='dodgerblue')\n    sns.lineplot(x=points, y=[0,0], color = 'black', label = \"std_dev\")\n    sns.scatterplot(x=[_min,_max], y=[0,0], color = 'orange', label = \"min/max\")\n    sns.scatterplot(x=[mean], y=[0], color = 'red', label = \"mean\")\n    sns.scatterplot(x=[median], y=[0], color = 'blue', label = \"median\")\n    plt.title(plot_title, fontsize=14)\n    plt.xlabel(x_label, fontsize=12)\n    plt.ylabel(y_label, fontsize=12)\n\n    # Creating a DataFrame for the descriptive statistics\n    variable_stats = pd.DataFrame({'Statistics': ['Minimum Value', 'Maximum Value', 'Range', 'Mean', \n                                                    'Median', 'Standard Deviation', 'Skewness', 'Kurtosis'], \n                                        'Value': [_min, _max, ran, mean, median, st_dev, skew, kurt]})\n    \n    plt.show()\n\n    display(tabulate(variable_stats, headers='keys', showindex=False, tablefmt='html'))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.879841Z","iopub.execute_input":"2023-05-21T21:12:34.880708Z","iopub.status.idle":"2023-05-21T21:12:34.893413Z","shell.execute_reply.started":"2023-05-21T21:12:34.880645Z","shell.execute_reply":"2023-05-21T21:12:34.892363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for plolting the distribution of categorical variables\ndef plot_cat(df, col, x_label, y_label, plot_title):\n    sns.set_style('darkgrid')\n    plt.figure(figsize=(12,8))\n    sns.countplot(data=df, x=col, color='dodgerblue')\n    plt.title(plot_title, fontsize=14)\n    plt.xlabel(x_label, fontsize=12)\n    plt.ylabel(y_label, fontsize=12)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.894582Z","iopub.execute_input":"2023-05-21T21:12:34.894907Z","iopub.status.idle":"2023-05-21T21:12:34.913059Z","shell.execute_reply.started":"2023-05-21T21:12:34.894878Z","shell.execute_reply":"2023-05-21T21:12:34.911616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Here Survived column is our target variable. Let's explore its distribution.</p>","metadata":{}},{"cell_type":"code","source":"# Plot distribution of Survived column\nplot_cat(df_train_copy, 'Survived', 'Survived', 'Count', 'Distribution of Survived column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:34.924285Z","iopub.execute_input":"2023-05-21T21:12:34.925076Z","iopub.status.idle":"2023-05-21T21:12:35.190715Z","shell.execute_reply.started":"2023-05-21T21:12:34.925028Z","shell.execute_reply":"2023-05-21T21:12:35.189380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting distribution of Pclass column\nplot_cat(df_train_copy, 'Pclass', 'Pclass', 'Count', 'Distribution of Pclass column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:35.192417Z","iopub.execute_input":"2023-05-21T21:12:35.193140Z","iopub.status.idle":"2023-05-21T21:12:35.396900Z","shell.execute_reply.started":"2023-05-21T21:12:35.193086Z","shell.execute_reply":"2023-05-21T21:12:35.396016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting distribution of Sex column\nplot_cat(df_train_copy, 'Sex', 'Sex', 'Count', 'Distribution of Sex column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:35.398284Z","iopub.execute_input":"2023-05-21T21:12:35.398899Z","iopub.status.idle":"2023-05-21T21:12:35.605173Z","shell.execute_reply.started":"2023-05-21T21:12:35.398862Z","shell.execute_reply":"2023-05-21T21:12:35.603420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plottting distribution of SibSp column\nplot_cat(df_train_copy, 'SibSp', 'SibSp', 'Count', 'Distribution of SibSp column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:35.606907Z","iopub.execute_input":"2023-05-21T21:12:35.607305Z","iopub.status.idle":"2023-05-21T21:12:35.854225Z","shell.execute_reply.started":"2023-05-21T21:12:35.607266Z","shell.execute_reply":"2023-05-21T21:12:35.852943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plottting distribution of Parch column\nplot_cat(df_train_copy, 'Parch', 'Parch', 'Count', 'Distribution of Parch column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:35.855772Z","iopub.execute_input":"2023-05-21T21:12:35.856391Z","iopub.status.idle":"2023-05-21T21:12:36.109007Z","shell.execute_reply.started":"2023-05-21T21:12:35.856351Z","shell.execute_reply":"2023-05-21T21:12:36.106707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plottting distribution of Embarked column\nplot_cat(df_train_copy, 'Embarked', 'Embarked', 'Count', 'Distribution of Embarked column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:36.111495Z","iopub.execute_input":"2023-05-21T21:12:36.111944Z","iopub.status.idle":"2023-05-21T21:12:36.415041Z","shell.execute_reply.started":"2023-05-21T21:12:36.111913Z","shell.execute_reply":"2023-05-21T21:12:36.413354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Lets explore the distribution of numerical variables.</p>","metadata":{}},{"cell_type":"code","source":"# Plotting distribution of Age column\nplot_dist(df_train, 'Age', 'Age', 'Count', 'Distribution of Age column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:36.417176Z","iopub.execute_input":"2023-05-21T21:12:36.418066Z","iopub.status.idle":"2023-05-21T21:12:37.191410Z","shell.execute_reply.started":"2023-05-21T21:12:36.418007Z","shell.execute_reply":"2023-05-21T21:12:37.189636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<ul>\n    <li>Median Age is 28.</li>\n    <li>Mean and Median Age are almost same. So Age is normally distributed.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# Let's see how Fare column is distributed\nplot_dist(df_train, 'Fare', 'Fare', 'Count', 'Distribution of Fare column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:37.192842Z","iopub.execute_input":"2023-05-21T21:12:37.193317Z","iopub.status.idle":"2023-05-21T21:12:37.735906Z","shell.execute_reply.started":"2023-05-21T21:12:37.193275Z","shell.execute_reply":"2023-05-21T21:12:37.734104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Kurtosis of Fare is very high. So Fare is highly skewed.</p>\n<p>Which is as expected, because certain classes will have higher fares than others and limited seats.</p>","metadata":{}},{"cell_type":"code","source":"# Let's create a new variable log_Fare by taking log of Fare column\ndf_train_copy['log_Fare'] = np.log(df_train_copy['Fare']+1)\ndf_test_copy['log_Fare'] = np.log(df_test_copy['Fare']+1)\n\n# Plotting distribution of log_Fare column\nplot_dist(df_train_copy, 'log_Fare', 'log_Fare', 'Count', 'Distribution of log_Fare column')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:37.737776Z","iopub.execute_input":"2023-05-21T21:12:37.738985Z","iopub.status.idle":"2023-05-21T21:12:38.247515Z","shell.execute_reply.started":"2023-05-21T21:12:37.738941Z","shell.execute_reply":"2023-05-21T21:12:38.245648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h4 id=\"sub_section1_2\">ii. Bivariate analysis</h4>\n    <p>Let's explore the relationship between the trip duration and other variables in the dataset:</p>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Function for plotting the distribution of numeric variables against the target variable\n# Here target variable is assumed to be categorical\ndef plot_num_vs_target(df, col, target, x_label, y_label, plot_title):\n    sns.set_style('darkgrid')\n    plt.figure(figsize=(12,8))\n    sns.boxplot(data=df, x=target, y=col, color='dodgerblue')\n    plt.title(plot_title, fontsize=14)\n    plt.xlabel(x_label, fontsize=12)\n    plt.ylabel(y_label, fontsize=12)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:38.250551Z","iopub.execute_input":"2023-05-21T21:12:38.251167Z","iopub.status.idle":"2023-05-21T21:12:38.259874Z","shell.execute_reply.started":"2023-05-21T21:12:38.251116Z","shell.execute_reply":"2023-05-21T21:12:38.258639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and Age\nplot_num_vs_target(df_train_copy, 'Age', 'Survived', 'Survived', 'Age', 'Relationship between Survived and Age')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:38.261535Z","iopub.execute_input":"2023-05-21T21:12:38.262273Z","iopub.status.idle":"2023-05-21T21:12:38.541838Z","shell.execute_reply.started":"2023-05-21T21:12:38.262226Z","shell.execute_reply":"2023-05-21T21:12:38.541006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and Fare\nplot_num_vs_target(df_train_copy, 'Fare', 'Survived', 'Survived', 'Fare', 'Relationship between Survived and Fare')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:38.542966Z","iopub.execute_input":"2023-05-21T21:12:38.544259Z","iopub.status.idle":"2023-05-21T21:12:38.794924Z","shell.execute_reply.started":"2023-05-21T21:12:38.544174Z","shell.execute_reply":"2023-05-21T21:12:38.793088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for plotting the distribution of categorical variables against the target variable\n# Here target variable and categorical variable are assumed to be categorical\ndef plot_cat_vs_target(df, col, target, x_label, y_label, plot_title):\n    sns.set_style('darkgrid')\n    plt.figure(figsize=(12,8))\n    sns.countplot(data=df, x=col, hue=target, palette='Set1')\n    plt.title(plot_title, fontsize=14)\n    plt.xlabel(x_label, fontsize=12)\n    plt.ylabel(y_label, fontsize=12)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:38.796457Z","iopub.execute_input":"2023-05-21T21:12:38.798082Z","iopub.status.idle":"2023-05-21T21:12:38.806330Z","shell.execute_reply.started":"2023-05-21T21:12:38.798028Z","shell.execute_reply":"2023-05-21T21:12:38.804559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and Pclass\nplot_cat_vs_target(df_train_copy, 'Pclass', 'Survived', 'Pclass', 'Count', 'Relationship between Survived and Pclass')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:38.808139Z","iopub.execute_input":"2023-05-21T21:12:38.808598Z","iopub.status.idle":"2023-05-21T21:12:39.273207Z","shell.execute_reply.started":"2023-05-21T21:12:38.808558Z","shell.execute_reply":"2023-05-21T21:12:39.270998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and Sex\nplot_cat_vs_target(df_train_copy, 'Sex', 'Survived', 'Sex', 'Count', 'Relationship between Survived and Sex')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:39.276086Z","iopub.execute_input":"2023-05-21T21:12:39.276566Z","iopub.status.idle":"2023-05-21T21:12:39.511986Z","shell.execute_reply.started":"2023-05-21T21:12:39.276522Z","shell.execute_reply":"2023-05-21T21:12:39.510791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and SibSp\nplot_cat_vs_target(df_train_copy, 'SibSp', 'Survived', 'SibSp', 'Count', 'Relationship between Survived and SibSp')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:39.514121Z","iopub.execute_input":"2023-05-21T21:12:39.514931Z","iopub.status.idle":"2023-05-21T21:12:39.857325Z","shell.execute_reply.started":"2023-05-21T21:12:39.514886Z","shell.execute_reply":"2023-05-21T21:12:39.854590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and Parch\nplot_cat_vs_target(df_train_copy, 'Parch', 'Survived', 'Parch', 'Count', 'Relationship between Survived and Parch')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:39.859037Z","iopub.execute_input":"2023-05-21T21:12:39.860308Z","iopub.status.idle":"2023-05-21T21:12:40.194317Z","shell.execute_reply.started":"2023-05-21T21:12:39.860264Z","shell.execute_reply":"2023-05-21T21:12:40.192901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and Embarked\nplot_cat_vs_target(df_train_copy, 'Embarked', 'Survived', 'Embarked', 'Count', 'Relationship between Survived and Embarked')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.195892Z","iopub.execute_input":"2023-05-21T21:12:40.199166Z","iopub.status.idle":"2023-05-21T21:12:40.462838Z","shell.execute_reply.started":"2023-05-21T21:12:40.199103Z","shell.execute_reply":"2023-05-21T21:12:40.461977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Survived and Name_Title\nplot_cat_vs_target(df_train_copy, 'Name_Title', 'Survived', 'Name_Title', 'Count', 'Relationship between Survived and Name_Title')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.464673Z","iopub.execute_input":"2023-05-21T21:12:40.465089Z","iopub.status.idle":"2023-05-21T21:12:40.757915Z","shell.execute_reply.started":"2023-05-21T21:12:40.465058Z","shell.execute_reply":"2023-05-21T21:12:40.756747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h3 id=\"section5\">5. Data Preprocessing</h3>\n    <p>Before we use variables in our model, we need to preprocess them. We will perform the following steps:</p>\n    <ul>\n        <li>One-hot encode categorical variables</li>\n        <li>Lable encode categorical variables</li>\n    </ul>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Function to encode categorical variables, we will use scikit-learn's LabelEncoder for label encoding and pandas get_dummies for one-hot encoding\nfrom sklearn.preprocessing import LabelEncoder\ndef encode_cat(df, col, encoding_type):\n    if encoding_type == 'label':\n        label_encoder = LabelEncoder()\n        df[col] = label_encoder.fit_transform(df[col])\n    elif encoding_type == 'onehot':\n        df = pd.get_dummies(df, columns=[col], prefix=[col])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.759260Z","iopub.execute_input":"2023-05-21T21:12:40.759610Z","iopub.status.idle":"2023-05-21T21:12:40.840536Z","shell.execute_reply.started":"2023-05-21T21:12:40.759579Z","shell.execute_reply":"2023-05-21T21:12:40.838640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check train data before encoding\ndf_train_copy.Pclass.value_counts(), df_train_copy.SibSp.value_counts(), df_train_copy.Parch.value_counts(), df_train_copy.Embarked.value_counts(), df_train_copy.Cabin.value_counts(), df_train_copy.Name_Title.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.842990Z","iopub.execute_input":"2023-05-21T21:12:40.843543Z","iopub.status.idle":"2023-05-21T21:12:40.876927Z","shell.execute_reply.started":"2023-05-21T21:12:40.843502Z","shell.execute_reply":"2023-05-21T21:12:40.875420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check test data after encoding\ndf_test_copy.Pclass.value_counts(), df_test_copy.SibSp.value_counts(), df_test_copy.Parch.value_counts(), df_test_copy.Embarked.value_counts(), df_test_copy.Cabin.value_counts(), df_test_copy.Name_Title.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.878046Z","iopub.execute_input":"2023-05-21T21:12:40.878334Z","iopub.status.idle":"2023-05-21T21:12:40.911623Z","shell.execute_reply.started":"2023-05-21T21:12:40.878307Z","shell.execute_reply":"2023-05-21T21:12:40.910200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding variables in the training dataset and create a new dataframe called df_train_encoded\ndf_train_encoded = df_train_copy.copy()\ndf_train_encoded = encode_cat(df_train_encoded, 'Survived', 'label')\ndf_train_encoded = encode_cat(df_train_encoded, 'Cabin', 'label')\ndf_train_encoded = encode_cat(df_train_encoded, 'Pclass', 'label')\ndf_train_encoded = encode_cat(df_train_encoded, 'Sex', 'onehot')\ndf_train_encoded = encode_cat(df_train_encoded, 'SibSp', 'label')\ndf_train_encoded = encode_cat(df_train_encoded, 'Parch', 'label')\ndf_train_encoded = encode_cat(df_train_encoded, 'Embarked', 'label')\ndf_train_encoded = encode_cat(df_train_encoded, 'Name_Title', 'label')\n\n# Encoding variables in the test dataset and create a new dataframe called df_test_encoded\ndf_test_encoded = df_test_copy.copy()\ndf_test_encoded = encode_cat(df_test_encoded, 'Cabin', 'label')\ndf_test_encoded = encode_cat(df_test_encoded, 'Pclass', 'label')\ndf_test_encoded = encode_cat(df_test_encoded, 'Sex', 'onehot')\ndf_test_encoded = encode_cat(df_test_encoded, 'SibSp', 'label')\ndf_test_encoded = encode_cat(df_test_encoded, 'Parch', 'label')\ndf_test_encoded = encode_cat(df_test_encoded, 'Embarked', 'label')\ndf_test_encoded = encode_cat(df_test_encoded, 'Name_Title', 'label')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.913976Z","iopub.execute_input":"2023-05-21T21:12:40.914321Z","iopub.status.idle":"2023-05-21T21:12:40.945122Z","shell.execute_reply.started":"2023-05-21T21:12:40.914292Z","shell.execute_reply":"2023-05-21T21:12:40.943330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check train data before encoding\ndf_train_encoded.Pclass.value_counts(), df_train_encoded.SibSp.value_counts(), df_train_encoded.Parch.value_counts(), df_train_encoded.Embarked.value_counts(), df_train_encoded.Cabin.value_counts(), df_train_encoded.Name_Title.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.947803Z","iopub.execute_input":"2023-05-21T21:12:40.948234Z","iopub.status.idle":"2023-05-21T21:12:40.966000Z","shell.execute_reply.started":"2023-05-21T21:12:40.948195Z","shell.execute_reply":"2023-05-21T21:12:40.963884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check train data before encoding\ndf_test_encoded.Pclass.value_counts(), df_test_encoded.SibSp.value_counts(), df_test_encoded.Parch.value_counts(), df_test_encoded.Embarked.value_counts(), df_test_encoded.Cabin.value_counts(), df_test_encoded.Name_Title.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.968117Z","iopub.execute_input":"2023-05-21T21:12:40.969155Z","iopub.status.idle":"2023-05-21T21:12:40.991599Z","shell.execute_reply.started":"2023-05-21T21:12:40.969112Z","shell.execute_reply":"2023-05-21T21:12:40.990268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Now that we have preprocessed the variables, let's check the correlation between them:</p>","metadata":{}},{"cell_type":"code","source":"# Function to plot correlation between variables\ndef plot_corr(df, size=10):\n    corr = df.corr()\n#     print(corr)\n    fig, ax = plt.subplots(figsize=(size, size))\n    sns.heatmap(corr, annot=True, linewidths=.5, ax=ax, cmap='crest')\n    plt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:40.996571Z","iopub.execute_input":"2023-05-21T21:12:40.998347Z","iopub.status.idle":"2023-05-21T21:12:41.010216Z","shell.execute_reply.started":"2023-05-21T21:12:40.998167Z","shell.execute_reply":"2023-05-21T21:12:41.008449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between variables in the training set\nplot_corr(df_train_encoded.drop(['PassengerId', 'Name', 'Ticket'], axis=1))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-05-21T21:12:41.012756Z","iopub.execute_input":"2023-05-21T21:12:41.013233Z","iopub.status.idle":"2023-05-21T21:12:42.034057Z","shell.execute_reply.started":"2023-05-21T21:12:41.013201Z","shell.execute_reply":"2023-05-21T21:12:42.032600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Let's check the correlation between the variables and the target variable:</p>","metadata":{}},{"cell_type":"code","source":"# Function to plot correlation of variables with the target variable as a barplot\ndef plot_corr_target(df, target, size=10):\n    corr = df.corr()\n    corr_target = corr[target]\n    corr_target = corr_target.sort_values(ascending=False)\n    corr_target = corr_target.drop(target)\n    plt.figure(figsize=(size, size))\n    corr_target.plot.barh()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.036384Z","iopub.execute_input":"2023-05-21T21:12:42.036898Z","iopub.status.idle":"2023-05-21T21:12:42.046537Z","shell.execute_reply.started":"2023-05-21T21:12:42.036858Z","shell.execute_reply":"2023-05-21T21:12:42.044076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check correlation of variables with the target variable\nplot_corr_target(df_train_encoded.drop(['Name', 'Ticket', 'PassengerId'], axis=1), 'Survived')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.049007Z","iopub.execute_input":"2023-05-21T21:12:42.050258Z","iopub.status.idle":"2023-05-21T21:12:42.420064Z","shell.execute_reply.started":"2023-05-21T21:12:42.050203Z","shell.execute_reply":"2023-05-21T21:12:42.418205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h3 id=\"section6\">6. Model Building</h3>\n    <p>Let's build a model to predict the Survival of passengers on the Titanic:</p>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# We will first separate the target variable from the features\ny = df_train_encoded['Survived']\nx = df_train_encoded.drop(['Survived', 'Name', 'Ticket', 'PassengerId'], axis=1)\nx.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.421430Z","iopub.execute_input":"2023-05-21T21:12:42.422925Z","iopub.status.idle":"2023-05-21T21:12:42.437206Z","shell.execute_reply.started":"2023-05-21T21:12:42.422840Z","shell.execute_reply":"2023-05-21T21:12:42.435081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Let's scale the features usinf scikit-learn's MinMax scaler:</p>","metadata":{}},{"cell_type":"code","source":"## Importing the MinMax Scaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.439094Z","iopub.execute_input":"2023-05-21T21:12:42.439603Z","iopub.status.idle":"2023-05-21T21:12:42.457844Z","shell.execute_reply.started":"2023-05-21T21:12:42.439562Z","shell.execute_reply":"2023-05-21T21:12:42.456170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.DataFrame(x_scaled, columns = x.columns)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.459824Z","iopub.execute_input":"2023-05-21T21:12:42.460251Z","iopub.status.idle":"2023-05-21T21:12:42.474476Z","shell.execute_reply.started":"2023-05-21T21:12:42.460211Z","shell.execute_reply":"2023-05-21T21:12:42.472048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data after scaling\nx.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.476168Z","iopub.execute_input":"2023-05-21T21:12:42.476494Z","iopub.status.idle":"2023-05-21T21:12:42.505577Z","shell.execute_reply.started":"2023-05-21T21:12:42.476465Z","shell.execute_reply":"2023-05-21T21:12:42.503951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Now, let's split the dataset into training and test sets:</p>","metadata":{}},{"cell_type":"code","source":"# Importing the train test split function\nfrom sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(x,y, random_state = 50 , stratify=y)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.507031Z","iopub.execute_input":"2023-05-21T21:12:42.510430Z","iopub.status.idle":"2023-05-21T21:12:42.607367Z","shell.execute_reply.started":"2023-05-21T21:12:42.510351Z","shell.execute_reply":"2023-05-21T21:12:42.605188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h3 id=\"section7\">7. Model Generation and Evaluation</h3>\n    <p>We will use different classification algorithms to build models and evaluate them using F1 score:</p>\n    <h4 id=\"sub_section2_1\">i. KNN Classifier</h4>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Import KNN classifier and metric F1 score\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.609216Z","iopub.execute_input":"2023-05-21T21:12:42.610129Z","iopub.status.idle":"2023-05-21T21:12:42.853522Z","shell.execute_reply.started":"2023-05-21T21:12:42.610057Z","shell.execute_reply":"2023-05-21T21:12:42.851537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Let's use KNN classifier to build a model and check consistency using cross validation:</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n# Function to cross validation for different values of k\n\ndef cross_val_knn(n_neighbors):\n    '''Takes in a value of k and returns the average and standard deviation of the F1 score for 10-fold cross validation'''\n    average = []\n    std = []\n    for i in n_neighbors:\n        knn = KNN(n_neighbors=i)\n        scores = cross_val_score(knn, train_x, train_y, cv=10, scoring='accuracy')\n        average.append(scores.mean())\n        std.append(scores.std())\n    return average, std","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.855488Z","iopub.execute_input":"2023-05-21T21:12:42.856061Z","iopub.status.idle":"2023-05-21T21:12:42.866040Z","shell.execute_reply.started":"2023-05-21T21:12:42.856010Z","shell.execute_reply":"2023-05-21T21:12:42.863138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the scores for a range of k values\nn_neighbors = range(1,50)\nmean, std = cross_val_knn(n_neighbors)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:42.869017Z","iopub.execute_input":"2023-05-21T21:12:42.869723Z","iopub.status.idle":"2023-05-21T21:12:47.342983Z","shell.execute_reply.started":"2023-05-21T21:12:42.869591Z","shell.execute_reply":"2023-05-21T21:12:47.340999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot the average F1 score for each value of k\nplt.plot(n_neighbors[10:20], mean[10:20], color = 'green', label = 'mean' )\nplt.xlabel('n_neighbors')\nplt.ylabel('Mean Score')\nplt.title('Mean Validation score')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.345380Z","iopub.execute_input":"2023-05-21T21:12:47.347194Z","iopub.status.idle":"2023-05-21T21:12:47.599105Z","shell.execute_reply.started":"2023-05-21T21:12:47.347116Z","shell.execute_reply":"2023-05-21T21:12:47.597636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot the standard deviation of the F1 score for each value of k\nplt.plot(n_neighbors[10:20], std[10:20], color = 'red', label = 'std' )\nplt.xlabel('n_neighbors')\nplt.ylabel('Standard Deviation')\nplt.title('Standard Deviation of Validation score')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.600660Z","iopub.execute_input":"2023-05-21T21:12:47.601042Z","iopub.status.idle":"2023-05-21T21:12:47.829176Z","shell.execute_reply.started":"2023-05-21T21:12:47.601008Z","shell.execute_reply":"2023-05-21T21:12:47.827283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try fiiting the model on the test set\nknn = KNN(n_neighbors=14)\nknn.fit(train_x, train_y)\n\n# Predict on the train set\nscore1 = knn.score(train_x, train_y)\n\n# Predict on the test set\nscore2 = knn.score(test_x, test_y)\n\nprint('Train score: ', score1)\nprint('Test score: ', score2)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.831580Z","iopub.execute_input":"2023-05-21T21:12:47.832072Z","iopub.status.idle":"2023-05-21T21:12:47.879528Z","shell.execute_reply.started":"2023-05-21T21:12:47.832037Z","shell.execute_reply":"2023-05-21T21:12:47.878232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.881057Z","iopub.execute_input":"2023-05-21T21:12:47.881353Z","iopub.status.idle":"2023-05-21T21:12:47.908338Z","shell.execute_reply.started":"2023-05-21T21:12:47.881326Z","shell.execute_reply":"2023-05-21T21:12:47.906341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_encoded_dropped = df_test_encoded.drop(['Name', 'Ticket', 'PassengerId'], axis=1)\ndf_test_encoded_dropped.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.910142Z","iopub.execute_input":"2023-05-21T21:12:47.910887Z","iopub.status.idle":"2023-05-21T21:12:47.932386Z","shell.execute_reply.started":"2023-05-21T21:12:47.910828Z","shell.execute_reply":"2023-05-21T21:12:47.930571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_scaled = scaler.fit_transform(df_test_encoded_dropped)\ndf_test_scaled = pd.DataFrame(test_scaled, columns= df_test_encoded_dropped.columns)\nsubmission_predictions = knn.predict(df_test_scaled)\n\n# train_x.shape, df_test_encoded.drop(['Name', 'Ticket', 'PassengerId'], axis=1).shape","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.934037Z","iopub.execute_input":"2023-05-21T21:12:47.935027Z","iopub.status.idle":"2023-05-21T21:12:47.969625Z","shell.execute_reply.started":"2023-05-21T21:12:47.934963Z","shell.execute_reply":"2023-05-21T21:12:47.967076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x.shape, test_x.shape, df_test_encoded_dropped.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.972071Z","iopub.execute_input":"2023-05-21T21:12:47.972578Z","iopub.status.idle":"2023-05-21T21:12:47.982276Z","shell.execute_reply.started":"2023-05-21T21:12:47.972475Z","shell.execute_reply":"2023-05-21T21:12:47.980086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_predictions","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:47.984492Z","iopub.execute_input":"2023-05-21T21:12:47.984969Z","iopub.status.idle":"2023-05-21T21:12:47.999102Z","shell.execute_reply.started":"2023-05-21T21:12:47.984923Z","shell.execute_reply":"2023-05-21T21:12:47.997862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['Survived'] = submission_predictions","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.000256Z","iopub.execute_input":"2023-05-21T21:12:48.001081Z","iopub.status.idle":"2023-05-21T21:12:48.011173Z","shell.execute_reply.started":"2023-05-21T21:12:48.000981Z","shell.execute_reply":"2023-05-21T21:12:48.008671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.013340Z","iopub.execute_input":"2023-05-21T21:12:48.014066Z","iopub.status.idle":"2023-05-21T21:12:48.030327Z","shell.execute_reply.started":"2023-05-21T21:12:48.014000Z","shell.execute_reply":"2023-05-21T21:12:48.028991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.031670Z","iopub.execute_input":"2023-05-21T21:12:48.032049Z","iopub.status.idle":"2023-05-21T21:12:48.047775Z","shell.execute_reply.started":"2023-05-21T21:12:48.032013Z","shell.execute_reply":"2023-05-21T21:12:48.045832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h4 id=\"sub_section2_2\">ii. Logistic Regression</h4>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Importing Logistic Regression\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.049496Z","iopub.execute_input":"2023-05-21T21:12:48.049891Z","iopub.status.idle":"2023-05-21T21:12:48.057239Z","shell.execute_reply.started":"2023-05-21T21:12:48.049860Z","shell.execute_reply":"2023-05-21T21:12:48.056059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating instance of Logistic Regression\nlog_reg = LogisticRegression()\n\n# Fitting the model\nlog_reg.fit(train_x, train_y)\n\n# Predicting over the Test Set and calculating F1\ntest_predict_log = log_reg.predict(test_x)\nk_log = accuracy_score(test_predict_log, test_y)\n\nprint('Accuracy Score    ', k_log )","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.058889Z","iopub.execute_input":"2023-05-21T21:12:48.059209Z","iopub.status.idle":"2023-05-21T21:12:48.087535Z","shell.execute_reply.started":"2023-05-21T21:12:48.059178Z","shell.execute_reply":"2023-05-21T21:12:48.086269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_predictions_log = log_reg.predict(df_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.089934Z","iopub.execute_input":"2023-05-21T21:12:48.090411Z","iopub.status.idle":"2023-05-21T21:12:48.102038Z","shell.execute_reply.started":"2023-05-21T21:12:48.090378Z","shell.execute_reply":"2023-05-21T21:12:48.099416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_predictions_log","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.105253Z","iopub.execute_input":"2023-05-21T21:12:48.106318Z","iopub.status.idle":"2023-05-21T21:12:48.117615Z","shell.execute_reply.started":"2023-05-21T21:12:48.106268Z","shell.execute_reply":"2023-05-21T21:12:48.116374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine predics with df_submission and save to csv\ndf_submission['Survived'] = submission_predictions_log\ndf_submission.to_csv('submission_log.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.119282Z","iopub.execute_input":"2023-05-21T21:12:48.119605Z","iopub.status.idle":"2023-05-21T21:12:48.134385Z","shell.execute_reply.started":"2023-05-21T21:12:48.119574Z","shell.execute_reply":"2023-05-21T21:12:48.133292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h4 id=\"sub_section2_3\">iii. Decision Tree Classifier</h4>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Importing Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.136223Z","iopub.execute_input":"2023-05-21T21:12:48.137629Z","iopub.status.idle":"2023-05-21T21:12:48.197113Z","shell.execute_reply.started":"2023-05-21T21:12:48.137572Z","shell.execute_reply":"2023-05-21T21:12:48.195638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating instance of Decision Tree Classifier\nclf = DecisionTreeClassifier(max_depth=4)\n\n# Fitting the model\nclf.fit(train_x, train_y)\n\n# Predict on the train set\nscore1 = clf.score(train_x, train_y)\n\n# Predict on the test set\nscore2 = clf.score(test_x, test_y)\n\n# Predicting over the Test Set and calculating F1\ntest_predict_dt = clf.predict(test_x)\nk_dt = accuracy_score(test_predict_dt, test_y)\n\nprint('Accuracy Score 1   ', score1 )\nprint('Accuracy Score 2   ', score2)\nprint('Accuracy Score    ', k_dt )","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.207962Z","iopub.execute_input":"2023-05-21T21:12:48.209626Z","iopub.status.idle":"2023-05-21T21:12:48.233386Z","shell.execute_reply.started":"2023-05-21T21:12:48.209555Z","shell.execute_reply":"2023-05-21T21:12:48.231195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_scaled.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.235228Z","iopub.execute_input":"2023-05-21T21:12:48.235643Z","iopub.status.idle":"2023-05-21T21:12:48.257469Z","shell.execute_reply.started":"2023-05-21T21:12:48.235599Z","shell.execute_reply":"2023-05-21T21:12:48.256239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_predictions_dt = clf.predict(df_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.258939Z","iopub.execute_input":"2023-05-21T21:12:48.260585Z","iopub.status.idle":"2023-05-21T21:12:48.272195Z","shell.execute_reply.started":"2023-05-21T21:12:48.260298Z","shell.execute_reply":"2023-05-21T21:12:48.268916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# Combine predictions with df_submission and save to csv\ndf_submission['Survived'] = submission_predictions_dt\ndf_submission.to_csv('submission_dt.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.273961Z","iopub.execute_input":"2023-05-21T21:12:48.274311Z","iopub.status.idle":"2023-05-21T21:12:48.288186Z","shell.execute_reply.started":"2023-05-21T21:12:48.274281Z","shell.execute_reply":"2023-05-21T21:12:48.286625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h4 id=\"sub_section2_4\">iv. Random Forest Classifier</h4>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Importing MLPC Classifier\nfrom sklearn.neural_network import MLPClassifier","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.289650Z","iopub.execute_input":"2023-05-21T21:12:48.290046Z","iopub.status.idle":"2023-05-21T21:12:48.328044Z","shell.execute_reply.started":"2023-05-21T21:12:48.290010Z","shell.execute_reply":"2023-05-21T21:12:48.326172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating instance of MLPC Classifier\nclf = MLPClassifier()\n\n# Fitting the model\nclf.fit(train_x, train_y)\n\n# Predicting over the Test Set and calculating F1\ntest_predict_mlpc = clf.predict(test_x)\nk_mlpc = accuracy_score(test_predict_mlpc, test_y)\n\nprint('Accuracy Score    ', k_mlpc )","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:48.330036Z","iopub.execute_input":"2023-05-21T21:12:48.330409Z","iopub.status.idle":"2023-05-21T21:12:49.061551Z","shell.execute_reply.started":"2023-05-21T21:12:48.330374Z","shell.execute_reply":"2023-05-21T21:12:49.060751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_predictions_mlpc = clf.predict(df_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:49.062668Z","iopub.execute_input":"2023-05-21T21:12:49.063370Z","iopub.status.idle":"2023-05-21T21:12:49.071155Z","shell.execute_reply.started":"2023-05-21T21:12:49.063337Z","shell.execute_reply":"2023-05-21T21:12:49.070217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine predictions with df_submission and save to csv\ndf_submission['Survived'] = submission_predictions_mlpc\ndf_submission.to_csv('submission_mlpc.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:49.072522Z","iopub.execute_input":"2023-05-21T21:12:49.073069Z","iopub.status.idle":"2023-05-21T21:12:49.083111Z","shell.execute_reply.started":"2023-05-21T21:12:49.073036Z","shell.execute_reply":"2023-05-21T21:12:49.082099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"col-md-8\">\n    <h4 id=\"sub_section2_5\">v. Support Vector Machine</h4>\n</div>\n<div class=\"col-md-4\">\n    <a href=\"#contents\">Back to top</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Importing Support Vector Classifier\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:49.085063Z","iopub.execute_input":"2023-05-21T21:12:49.085838Z","iopub.status.idle":"2023-05-21T21:12:49.096756Z","shell.execute_reply.started":"2023-05-21T21:12:49.085799Z","shell.execute_reply":"2023-05-21T21:12:49.095635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating instance of Support Vector Classifier\nclf = SVC()\n\n# Fitting the model\nclf.fit(train_x, train_y)\n\n# Predicting over the Test Set and calculating F1\ntest_predict_svc = clf.predict(test_x)\nk_svc = accuracy_score(test_predict_svc, test_y)\n\nprint('Accuracy Score    ', k_svc )","metadata":{"execution":{"iopub.status.busy":"2023-05-21T21:12:49.098472Z","iopub.execute_input":"2023-05-21T21:12:49.099168Z","iopub.status.idle":"2023-05-21T21:12:49.136891Z","shell.execute_reply.started":"2023-05-21T21:12:49.099133Z","shell.execute_reply":"2023-05-21T21:12:49.136011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}